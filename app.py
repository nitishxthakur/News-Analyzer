# -*- coding: utf-8 -*-
"""seprate_model_for_each_bias_topic_leaning_biasedwords_roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EeiL3JhjpCC1kBLGj_TfURhh79OhSp3u
"""
# Import libraries
import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.preprocessing import LabelEncoder
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments

# Load data
df = pd.read_excel('cleaned_babe_dataset_updated.xlsx', sheet_name='Sheet1')

# Encode labels
bias_mapping = {'Non biased': 0, 'Biased': 1}
df['label_bias_encoded'] = df['label_bias'].map(bias_mapping)

topic_encoder = LabelEncoder()
df['topic_encoded'] = topic_encoder.fit_transform(df['topic'])

leaning_encoder = LabelEncoder()
df['leaning_encoded'] = leaning_encoder.fit_transform(df['type'])

# Load tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

"""**Bias Detection**"""

# Prepare data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['label_bias_encoded'].tolist(), test_size=0.2, random_state=42, stratify=df['label_bias_encoded']
)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# Load model
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)

# Evaluation metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}

# Training arguments
training_args = TrainingArguments(
    output_dir='./results_bias_detection',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none"
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

"""**Topic Detection**"""

# Prepare data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['topic_encoded'].tolist(), test_size=0.2, random_state=42, stratify=df['topic_encoded']
)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# Load model
num_labels = len(df['topic_encoded'].unique())
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

# Update compute_metrics for multi-class
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}

# Training arguments
training_args.output_dir = './results_topic_detection'

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

"""**Biased Words Extraction**"""

from transformers import RobertaForTokenClassification
from transformers import RobertaTokenizerFast
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

# Load fast tokenizer (already done above)
# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

# Function to align labels
def tokenize_and_align_labels(text, biased_words_list):
    tokenized_inputs = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=512,
        return_offsets_mapping=True,
        return_tensors='pt'
    )
    labels = []
    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])

    for token in tokens:
        # Check if token corresponds to any biased word
        if token in ['[CLS]', '[SEP]', '[PAD]']:
            labels.append(-100)  # Ignore special tokens
        else:
            # Basic logic: if token matches any word (rough match, later improve)
            clean_token = token.lstrip('Ä ')  # RoBERTa uses Ä  for space
            if clean_token.lower() in [bw.lower() for bw in biased_words_list]:
                labels.append(1)  # Biased word
            else:
                labels.append(0)  # Non-biased

    tokenized_inputs['labels'] = torch.tensor(labels)
    return tokenized_inputs

# Now process the dataset
tokenized_datasets = [tokenize_and_align_labels(row['text'], eval(row['biased_words'])) for idx, row in df.iterrows()]

class TokenDataset(Dataset):
    def __init__(self, encodings):
        self.input_ids = torch.stack([batch['input_ids'].squeeze(0) for batch in encodings])
        self.attention_mask = torch.stack([batch['attention_mask'].squeeze(0) for batch in encodings])
        self.labels = torch.stack([batch['labels'] for batch in encodings])

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

    def __len__(self):
        return len(self.input_ids)

dataset = TokenDataset(tokenized_datasets)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Load token classification model
model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=2)

# Metrics
def compute_metrics_token(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)

    true_labels = labels.flatten()
    pred_labels = preds.flatten()

    # No .cpu() needed now
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')
    acc = accuracy_score(true_labels, pred_labels)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}

# Training arguments
training_args.output_dir = './results_biased_words_extraction'

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics_token,
)

# Train the model
trainer.train()

"""**Leaning Detection**"""

# Prepare data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['leaning_encoded'].tolist(), test_size=0.2, random_state=42, stratify=df['leaning_encoded']
)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# Load model
num_labels = len(df['leaning_encoded'].unique())
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

# Reuse compute_metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}

# Training arguments
training_args.output_dir = './results_leaning_detection'

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# Train
trainer.train()


# Import required libraries
import streamlit as st
import torch
from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, RobertaForTokenClassification

# Load tokenizer
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

# Load trained models
bias_model = RobertaForSequenceClassification.from_pretrained('./results_bias_detection/checkpoint-665')
topic_model = RobertaForSequenceClassification.from_pretrained('./results_topic_detection/checkpoint-665')
biased_word_model = RobertaForTokenClassification.from_pretrained('./results_biased_words_extraction/checkpoint-665')
leaning_model = RobertaForSequenceClassification.from_pretrained('./results_leaning_detection/checkpoint-665')

# Labels used during training
topic_labels = [
    'abortion', 'black lives matter', 'coronavirus', 'elections 2020', 'environment',
    'gender', 'gun control', 'immigration', 'international politics and world news', 'metoo',
    'middle class', 'sport', 'student debt', 'taxes', 'trump presidency',
    'universal health care', 'vaccines', 'white nationalism'
]
leaning_labels = ['Left', 'Center', 'Right']

# Function to analyze the article
def analyze_article(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")

    # 1. Bias Detection
    bias_output = bias_model(**inputs)
    is_biased = torch.argmax(bias_output.logits, dim=1).item()
    bias_result = 'ðŸŸ¥ Biased' if is_biased == 1 else 'âœ… Unbiased'

    # 2. Topic Detection
    topic_output = topic_model(**inputs)
    topic_index = torch.argmax(topic_output.logits, dim=1).item()
    topic_result = topic_labels[topic_index] if topic_index < len(topic_labels) else "Unknown Topic"

    # 3. Biased Word Detection
    word_inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding="max_length")
    word_output = biased_word_model(**word_inputs)
    word_preds = torch.argmax(word_output.logits, dim=2).squeeze(0).tolist()
    tokens = tokenizer.convert_ids_to_tokens(word_inputs['input_ids'].squeeze(0))
    biased_words = [tokens[i] for i in range(len(tokens)) if word_preds[i] == 1]

    # 4. Leaning Detection
    leaning_output = leaning_model(**inputs)
    leaning_index = torch.argmax(leaning_output.logits, dim=1).item()
    leaning_result = leaning_labels[leaning_index] if leaning_index < len(leaning_labels) else "Unknown Leaning"

    return bias_result, topic_result, biased_words, leaning_result

# Streamlit Interface
st.title("Article Bias and Topic Analysis")
st.write("Paste an article below to analyze its bias, topic, leaning, and biased words.")

# Input text area
article = st.text_area("Paste your article here:", height=200)

# Analyze button
if st.button("Analyze Article"):
    if article.strip():
        with st.spinner("Analyzing..."):
            bias, topic, biased_words, leaning = analyze_article(article)

        # Display results
        st.subheader("Analysis Results")
        st.write(f"**Bias:** {bias}")
        st.write(f"**Topic:** {topic}")
        st.write(f"**Political Leaning:** {leaning}")
        st.write(f"**Biased Words:** {', '.join(biased_words) if biased_words else 'None'}")
    else:
        st.warning("Please paste an article to analyze.")

# Run with `streamlit run app.py`
